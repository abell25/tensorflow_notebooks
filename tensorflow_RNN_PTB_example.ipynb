{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "#!tar xvf simple-examples.tgz\n",
    "#!rm simple-examples.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-train\t\t   5-one-iter\t\t       9-char-based-lm\ttemp\r\n",
      "2-nbest-rescore    6-recovery-during-training  data\r\n",
      "3-combination\t   7-dynamic-evaluation        models\r\n",
      "4-data-generation  8-direct\t\t       rnnlm-0.2b\r\n"
     ]
    }
   ],
   "source": [
    "!ls simple-examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SmallConfig(object):\n",
    "  \"\"\"Small config.\"\"\"\n",
    "  init_scale = 0.1\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 25\n",
    "  hidden_size = 200\n",
    "  max_epoch = 4\n",
    "  max_max_epoch = 13\n",
    "  keep_prob = 1.0\n",
    "  lr_decay = 0.5\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class MediumConfig(object):\n",
    "  \"\"\"Medium config.\"\"\"\n",
    "  init_scale = 0.05\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 5\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 650\n",
    "  max_epoch = 6\n",
    "  max_max_epoch = 39\n",
    "  keep_prob = 0.5\n",
    "  lr_decay = 0.8\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000\n",
    "\n",
    "\n",
    "class LargeConfig(object):\n",
    "  \"\"\"Large config.\"\"\"\n",
    "  init_scale = 0.04\n",
    "  learning_rate = 1.0\n",
    "  max_grad_norm = 10\n",
    "  num_layers = 2\n",
    "  num_steps = 35\n",
    "  hidden_size = 1500\n",
    "  max_epoch = 14\n",
    "  max_max_epoch = 55\n",
    "  keep_prob = 0.35\n",
    "  lr_decay = 1 / 1.15\n",
    "  batch_size = 20\n",
    "  vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.models.rnn.ptb import reader\n",
    "import time\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = reader.ptb_raw_data(\"./simple-examples/data/\")\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "# Display the data\n",
    "#!head -n6 simple-examples/data/ptb.train.txt\n",
    "#[(len(x),x[:10]) if type(x) == list else (x,) for x in raw_data[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = SmallConfig()\n",
    "\n",
    "input_data_train, targets_train = reader.ptb_producer(train_data, config.batch_size, config.num_steps, name=\"train_input\")\n",
    "input_data_test, targets_test = reader.ptb_producer(test_data, config.batch_size, config.num_steps, name=\"test_input\")\n",
    "input_data_valid, targets_valid = reader.ptb_producer(valid_data, config.batch_size, config.num_steps, name=\"valid_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NlpRnnModel:\n",
    "    def __init__(self, input_data, targets, name=None):\n",
    "        #input_data, targets = input_data_train, targets_train\n",
    "\n",
    "        #Create the RNN cell\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(config.hidden_size, forget_bias=0.0, state_is_tuple=True)\n",
    "        if config.keep_prob < 1.0:\n",
    "            lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.num_layers, state_is_tuple=True)\n",
    "        \n",
    "        initial_state = cell.zero_state(config.batch_size, tf.float32)\n",
    "\n",
    "        embedding = tf.get_variable(\"embedding\", [config.vocab_size, config.hidden_size], dtype=tf.float32)\n",
    "        inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "        # Run cell for each time step\n",
    "        outputs = [] \n",
    "        state = initial_state\n",
    "        with tf.variable_scope(\"RNN_loop\"):\n",
    "            for time_step in range(config.num_steps):\n",
    "                if time_step > 0: tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:, time_step, :], state) # input:[20, 200], output:[20, 200]\n",
    "                outputs.append(cell_output)\n",
    "\n",
    "        # Convert RNN output to logits\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1, config.hidden_size]) # [20*25, 200]\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [config.hidden_size, config.vocab_size], dtype=tf.float32) # [200, 10000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [config.vocab_size], dtype=tf.float32) # [10000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b #[20*25, 10,000]\n",
    "\n",
    "        # Calc loss function and optimize it\n",
    "        loss = tf.nn.seq2seq.sequence_loss_by_example(\n",
    "            [logits],\n",
    "            [tf.reshape(targets, [-1])],\n",
    "            [tf.ones([config.batch_size * config.num_steps], dtype=tf.float32)])\n",
    "        cost = tf.reduce_sum(loss) / config.batch_size\n",
    "        final_state = state\n",
    "\n",
    "        learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "\n",
    "        new_learning_rate = tf.placeholder(tf.float32, shape=[], name=\"new_learning_rate\")\n",
    "        learning_rate_update = tf.assign(learning_rate, new_learning_rate)\n",
    "        \n",
    "        self.cell = cell\n",
    "        self.initial_state = initial_state\n",
    "        self.final_state = final_state\n",
    "        self.loss = loss\n",
    "        self.cost = cost\n",
    "        self.optimizer = optimizer\n",
    "        self.train_op = train_op\n",
    "        self.learning_rate = learning_rate\n",
    "        self.new_learning_rate = new_learning_rate\n",
    "        self.tvars = tvars\n",
    "        self.grads = grads\n",
    "        self.outputs = outputs\n",
    "        self.output = output\n",
    "        \n",
    "def run_epoch(sess, model, epoch_size, eval_op=None, verbose=False):\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    state = sess.run(model.initial_state)\n",
    "\n",
    "    for step in range(epoch_size):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(model.initial_state):\n",
    "            feed_dict[c] = state[i].c\n",
    "            feed_dict[h] = state[i].h\n",
    "\n",
    "        fetches = {\"cost\": model.cost, \"final_state\": model.final_state}\n",
    "        if eval_op is not None: fetches[\"eval_op\"] = eval_op\n",
    "\n",
    "        vals = sess.run(fetches, feed_dict)\n",
    "        cost = vals[\"cost\"]\n",
    "        state = vals[\"final_state\"]\n",
    "\n",
    "        costs += cost\n",
    "        iters += config.num_steps\n",
    "\n",
    "        if verbose and step % (epoch_size / 10) == 10:\n",
    "            print(\"%.3f perplexity: %.3f speed: %.0f wps\" %\n",
    "                (step * 1.0 / epoch_size, np.exp(costs / iters),\n",
    "                 iters * config.batch_size / (time.time() - start_time)))\n",
    "\n",
    "    avg_cost = np.exp(costs / iters)\n",
    "    return avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'train_input/Slice:0' shape=(20, 25) dtype=int32>,\n",
       " <tf.Tensor 'embedding_lookup:0' shape=(20, 25, 200) dtype=float32>,\n",
       " (25,\n",
       "  <tf.Tensor 'RNN_loop/MultiRNNCell/Cell1/BasicLSTMCell/mul_2:0' shape=(20, 200) dtype=float32>),\n",
       " <tf.Tensor 'Reshape:0' shape=(500, 200) dtype=float32>,\n",
       " (<tf.Tensor 'train_input/Slice_1:0' shape=(20, 25) dtype=int32>,\n",
       "  <tf.Tensor 'Reshape_3:0' shape=(500,) dtype=int32>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data, inputs, (len(outputs), outputs[0]), output, (targets, tf.reshape(targets, [-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "with tf.variable_scope(\"Model\", reuse=None, initializer=initializer):\n",
    "    train_model = NlpRnnModel(input_data_train, targets_train, \"train_model\")\n",
    "with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "    test_model = NlpRnnModel(input_data_test, targets_test, \"test_model\")\n",
    "with tf.variable_scope(\"Model\", reuse=True, initializer=initializer):\n",
    "    valid_model = NlpRnnModel(input_data_valid, targets_valid, \"valid_model\")\n",
    "\n",
    "def calc_epoch_size(data, batch_size, num_steps):\n",
    "    return ((len(train_data) // batch_size) - 1) // num_steps\n",
    "\n",
    "train_epoch_size = calc_epoch_size(train_data, config.batch_size, config.num_steps)\n",
    "test_epoch_size = calc_epoch_size(test_data, config.batch_size, config.num_steps)\n",
    "valid_epoch_size = calc_epoch_size(valid_data, config.batch_size, config.num_steps)\n",
    "    \n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0\n",
      "Epoch: 1 Learning rate: 0.000\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(\"iter {}\".format(i))\n",
    "    \n",
    "    learning_rate_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n",
    "    print(\"Epoch: %d Learning rate: %.3f\" % (i + 1, sess.run(train_model.learning_rate)))\n",
    "\n",
    "    train_perplexity = run_epoch(sess, train_model, train_epoch_size, eval_op=train_model.train_op, verbose=True)\n",
    "    print(\"Epoch: %d Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "\n",
    "    valid_perplexity = run_epoch(sess, valid_model, valid_epoch_size)\n",
    "    print(\"Epoch: %d Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "\n",
    "    test_perplexity = run_epoch(sess, test_model, test_epoch_size)\n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NlpRnnModel instance at 0x7f2105596cb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
