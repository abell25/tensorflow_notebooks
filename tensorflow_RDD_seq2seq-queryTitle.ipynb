{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "from tensorflow.python.ops import seq2seq\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "from tensorflow.models.rnn.translate import seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3\n",
    "\n",
    "class VocabGenerator:\n",
    "    def __init__(self, tokenizer=None, special_chars=['PAD_ID', 'GO_ID', 'EOS_ID', 'UNK_ID']):\n",
    "        if tokenizer is None:\n",
    "            tokenizer = lambda s: [w for w in re.split(\" +\", s.lower()) if len(w.strip()) > 0]\n",
    "        self.tokenizer = tokenizer \n",
    "        self.special_chars = special_chars\n",
    "        \n",
    "        self.sent_set = set()\n",
    "        self.word_counts = {}\n",
    "            \n",
    "    def processSent(self, sent):\n",
    "        if hash(sent) in self.sent_set:\n",
    "            return\n",
    "        \n",
    "        self.sent_set.add(hash(sent))\n",
    "        for word in self.tokenizer(sent):\n",
    "            if len(word.strip()) > 0:\n",
    "                self.word_counts[word] = 0 if word not in self.word_counts else self.word_counts[word]+1\n",
    "            \n",
    "    def generateVocab(self, sents=[], vocab_size=10000):\n",
    "        for sent in sents:\n",
    "            self.processSent(sent)\n",
    "\n",
    "        vocab_remaining = vocab_size - len(self.special_chars)\n",
    "        top_word_counts = sorted([(w, self.word_counts[w]) for w in self.word_counts], key=lambda x: x[1], reverse=True)[:vocab_remaining]\n",
    "        self.vocab = self.special_chars + [w for w,c in top_word_counts]\n",
    "        self.vocab_lookup = {w: i for i,w in enumerate(self.vocab)}\n",
    "        return self.vocab\n",
    "    \n",
    "    def setVocab(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_lookup = {w: i for i,w in enumerate(self.vocab)}\n",
    "        \n",
    "    def saveVocab(self, vocab_file, vocab=None):\n",
    "        vocab = vocab if vocab is not None else self.vocab\n",
    "        with open(vocab_file, \"w\") as out:\n",
    "            for w in vocab:\n",
    "                print(w, file=out)\n",
    "                \n",
    "    def loadVocab(self, vocab_file):\n",
    "        vocab = [line.strip(\"\\n\") for line in open(vocab_file)]\n",
    "        self.setVocab(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def sentToIndexes(self, sent):\n",
    "        #return [self.vocab_lookup[w] if w in self.vocab_lookup else -1 for w in self.tokenizer(sent)]\n",
    "        words = self.tokenizer(sent)\n",
    "        indexes = [self.vocab_lookup[w] if w in self.vocab_lookup else self.vocab_lookup['UNK_ID'] for w in  words]\n",
    "        return indexes if len(indexes) > 0 else [self.vocab_lookup['UNK_ID']]\n",
    "    \n",
    "    def indexesToSents(self, indexes):\n",
    "        return \" \".join([self.vocab[i] for i in indexes])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class QueryTitleLoader:\n",
    "    def __init__(self, path, vocab_dir, max_query_len=6, max_title_len=16):\n",
    "        self.path = path\n",
    "        self.vocab_dir = vocab_dir\n",
    "        self.max_query_len = max_query_len\n",
    "        self.max_title_len = max_title_len\n",
    "        self.getQueryTitles()\n",
    "        \n",
    "        self.queryVocabGenerator, self.titleVocabGenerator = VocabGenerator(), VocabGenerator()\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(line):\n",
    "        line = line.lower().strip()\n",
    "        line = re.sub(\"[-/\\\\\\\\]\", \" \", line)\n",
    "        line = re.sub(\"[^-\\w\\d \\.\\t]\", \" \", line)\n",
    "        line = re.sub(\"  +\", \" \", line)\n",
    "        return line\n",
    "\n",
    "    @staticmethod\n",
    "    def parseLine(line):\n",
    "        vals = line.strip(\"\\n \").split(\"\\t\")\n",
    "        query, title = vals[0], vals[2]\n",
    "        return (query, title)\n",
    "\n",
    "    def queryTitleFilter(self, query, title):\n",
    "        negative_match = \"-\\(|^-\\w| -\\w\"\n",
    "        chars_to_reject = \"[\\(\\)\\[\\]\\\"]\"\n",
    "        num_only = \"^-?[0-9\\.]+$\"\n",
    "        any_number = \"-?[0-9\\.]{2,}\"\n",
    "        if len(query.split(\" \")) > self.max_query_len or len(title.split(\" \")) > self.max_title_len:\n",
    "            return False\n",
    "        if len(query.split(\" \")) < 1 or len(query.strip()) == 0:\n",
    "            return False\n",
    "        if len(re.findall(\"|\".join([negative_match, chars_to_reject, num_only, any_number]), query)) > 0:\n",
    "            return False   \n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def writeLines(ar, filename):\n",
    "        with open(filename, \"w\") as out:\n",
    "             for i, x in enumerate(ar):\n",
    "                print(x, file=out)\n",
    "                \n",
    "    def getQueryTitles(self):\n",
    "        queryTitlePairsRaw = (QueryTitleLoader.parseLine(line) for f in os.listdir(self.path) for line in open(os.path.join(self.path, f), \"r\"))\n",
    "        queryTitlePairs = ((QueryTitleLoader.normalize(q), QueryTitleLoader.normalize(t)) for q,t in queryTitlePairsRaw if self.queryTitleFilter(q,t))\n",
    "        self.queryTitlePairs = queryTitlePairs\n",
    "        return self.queryTitlePairs\n",
    "    \n",
    "    def getQueryTitlesInts(self):\n",
    "        return (self.queryTitleToIndexes(q, t) for q,t in self.getQueryTitles())\n",
    "    \n",
    "    def getQueryTitlesBatch(self, num_records): \n",
    "        batch = []\n",
    "        for k in range(num_records):\n",
    "            try:\n",
    "                batch.append(next(self.queryTitlePairs))\n",
    "            except StopIteration:\n",
    "                self.getQueryTitles()\n",
    "        return batch\n",
    "    \n",
    "    def queryTitleToIndexes(self, query, title):\n",
    "        return (self.queryVocabGenerator.sentToIndexes(query), self.titleVocabGenerator.sentToIndexes(title))\n",
    "    \n",
    "    def getQueryTitlesIntBatch(self, num_records): \n",
    "        batch = self.getQueryTitlesBatch(num_records)\n",
    "        return [self.queryTitleToIndexes(q,t) for q,t in batch]\n",
    "    \n",
    "    def generateVocab(self, num_examples=1000000, query_vocab_size=10000, title_vocab_size=10000):        \n",
    "        for q,t in self.getQueryTitlesBatch(num_examples):\n",
    "            self.queryVocabGenerator.processSent(q)\n",
    "            self.titleVocabGenerator.processSent(t)\n",
    "\n",
    "        self.q_vocab = self.queryVocabGenerator.generateVocab(vocab_size=query_vocab_size)\n",
    "        self.t_vocab = self.titleVocabGenerator.generateVocab(vocab_size=title_vocab_size)\n",
    "        \n",
    "        return (self.q_vocab, self.t_vocab)\n",
    "    \n",
    "    def setVocab(self, queryVocab, titleVocab):\n",
    "        self.q_vocab = self.queryVocabGenerator.setVocab(queryVocab)\n",
    "        self.t_vocab = self.titleVocabGenerator.generateVocab(titleVocab)\n",
    "        \n",
    "    def saveVocab(self):\n",
    "        self.queryVocabGenerator.saveVocab(os.path.join(self.vocab_dir, \"query_vocab.txt\"))\n",
    "        self.titleVocabGenerator.saveVocab(os.path.join(self.vocab_dir, \"title_vocab.txt\"))\n",
    "    \n",
    "    def loadVocab(self):\n",
    "        self.q_vocab = self.queryVocabGenerator.loadVocab(os.path.join(self.vocab_dir, \"query_vocab.txt\"))\n",
    "        self.t_vocab = self.titleVocabGenerator.loadVocab(os.path.join(self.vocab_dir, \"query_vocab.txt\"))\n",
    "        \n",
    "\n",
    "    \n",
    "#queryTitlePairLoader = QueryTitleLoader(\"/Users/anthbell/Data/queryClickPairs/train\", max_query_len=6, max_title_len=16)\n",
    "#query_vocab, title_vocab = queryTitlePairLoader.generateVocab(num_examples=100000, query_vocab_size=1000, title_vocab_size=1000)   \n",
    "#i = queryTitlePairLoader.queryVocabGenerator.sentToIndexes(\"angry birds hat\")\n",
    "#s = queryTitlePairLoader.queryVocabGenerator.indexesToSents(i)\n",
    "#print(i, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bucket_id=0\n",
    "\n",
    "class QueryTitleRNNGenerator:\n",
    "    def __init__(self, checkpoint_dir, train_loader, test_loader, num_layers, size, src_vocab_size, dest_vocab_size, max_grad_clip, batch_size, learning_rate, learning_rate_decay_factor):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.size = size\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.dest_vocab_size = dest_vocab_size\n",
    "        self.max_grad_clip = max_grad_clip\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay_factor = learning_rate_decay_factor\n",
    "        \n",
    "        self.steps_per_checkpoint = 20\n",
    "        \n",
    "        \n",
    "    def load_data(self, data_dir):\n",
    "        self.queryTitlePairLoader = QueryTitleLoader(data_dir, max_query_len=6, max_title_len=16)\n",
    "        queryTitlePairLoader.generateVocab(num_examples=10000, query_vocab_size=self.src_vocab_size, title_vocab_size=self.dest_vocab_size)\n",
    "        return [self.queryTitlePairLoader.getQueryTitlesIntBatch(10000)]\n",
    "       \n",
    "    def create_model(self, session, forward_only):\n",
    "        model = seq2seq_model.Seq2SeqModel(\n",
    "            self.src_vocab_size,\n",
    "            self.dest_vocab_size,\n",
    "            [(6,16)],\n",
    "            self.size,\n",
    "            self.num_layers,\n",
    "            self.max_grad_clip,\n",
    "            self.batch_size,\n",
    "            self.learning_rate,\n",
    "            self.learning_rate_decay_factor,\n",
    "            forward_only=forward_only,\n",
    "            dtype=tf.float32)\n",
    "        ckpt = tf.train.get_checkpoint_state(self.checkpoint_dir)\n",
    "        if ckpt and tf.gfile.Exists(ckpt.model_checkpoint_path):\n",
    "            print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "            model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"Created model with fresh parameters.\")\n",
    "            session.run(tf.initialize_all_variables())\n",
    "        return model\n",
    "    \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            model = self.create_model(sess, False)\n",
    "            self.model = model\n",
    "\n",
    "            #train_set = self.train_loader.getQueryTitlesIntBatch(20000000)\n",
    "            #test_set = self.test_loader.getQueryTitlesIntBatch(5000000)\n",
    "\n",
    "            # This is the training loop.\n",
    "            step_time, loss = 0.0, 0.0\n",
    "            current_step = 0\n",
    "            self.previous_losses = []\n",
    "            self.train_perplexity = []\n",
    "            self.test_perplexity = []\n",
    "            self.learning_rates = []\n",
    "            self.step_sizes = []\n",
    "            while True:\n",
    "                start_time = time.time()\n",
    "                train_set = [self.train_loader.getQueryTitlesIntBatch(self.batch_size*2)]\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch(train_set, bucket_id)\n",
    "                _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)\n",
    "                step_time += (time.time() - start_time) / self.steps_per_checkpoint\n",
    "                loss += step_loss / self.steps_per_checkpoint\n",
    "                current_step += 1\n",
    "\n",
    "                self.train_perplexity.append(math.exp(float(loss)) if loss < 300 else float(\"inf\"))\n",
    "                if current_step % self.steps_per_checkpoint == 0:\n",
    "                    perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n",
    "                    print(\"global step {} learning rate {:.4f} step-time {:.2f} perplexity {:.2f}\".format(\n",
    "                            model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n",
    "                    \n",
    "                    #Print some example outputs\n",
    "                    batch = self.test_loader.getQueryTitlesBatch(100)\n",
    "                    test_set = [batch[k] for k in random.choice(range(len(batch)), 5, replace=False)]\n",
    "                    results = self.decodeWithModel(sess, model, [q for q,t in test_set])\n",
    "                    print(\"results: \")\n",
    "                    for t, (q,new_t) in zip([t for q,t in test_set], results):\n",
    "                        print(\"[{:20}]  [{:40}] [{:40}]\".format(q[:25], t[:40], new_t[:40]))\n",
    "                    \n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(self.previous_losses) > 2 and loss > max(self.previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                    \n",
    "                self.learning_rates.append(model.learning_rate.eval())\n",
    "                self.step_sizes.append(model.global_step.eval())\n",
    "                    \n",
    "                self.previous_losses.append(loss)\n",
    "\n",
    "                checkpoint_path = os.path.join(self.checkpoint_dir, \"translate.ckpt\")\n",
    "                model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "                \n",
    "                step_time, loss = 0.0, 0.0\n",
    "                \n",
    "                # Run evals on development set and print their perplexity.\n",
    "                test_set = [self.test_loader.getQueryTitlesIntBatch(self.batch_size*2)]\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch(test_set, bucket_id)\n",
    "                _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "                \n",
    "                eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
    "                self.test_perplexity.append(eval_ppx)\n",
    "                print(\"  eval: perplexity {:.2f}\".format(eval_ppx))\n",
    "                \n",
    "        \n",
    "    def decodeQuery(self, indexes):\n",
    "        return self.train_loader.queryVocabGenerator.indexesToSents(indexes)\n",
    "    \n",
    "    def decodeTitle(self, indexes):\n",
    "        return self.train_loader.titleVocabGenerator.indexesToSents(indexes)\n",
    "        \n",
    "    def encodeQuery(self, query):\n",
    "        return self.train_loader.queryVocabGenerator.sentToIndexes(query)\n",
    "        \n",
    "    def encodeTitle(self, title):\n",
    "        return self.train_loader.titleVocabGenerator.sentToIndexes(title)\n",
    "       \n",
    "    def decodeWithModel(self, sess, model, sentences):\n",
    "        batch_size = model.batch_size\n",
    "        model.batch_size = 1  \n",
    "\n",
    "        output_sents = []\n",
    "        for sentence in sentences:\n",
    "            #print(\"sentence: {}\".format(sentence))\n",
    "            token_ids = self.encodeQuery(sentence)\n",
    "\n",
    "            #print(\"sent: {}, token_ids: {}, bucket_id: {}\".format(sentence, token_ids, bucket_id))\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(token_ids, [])]}, bucket_id)\n",
    "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "\n",
    "            if EOS_ID in outputs:\n",
    "                outputs = outputs[:outputs.index(EOS_ID)]\n",
    "\n",
    "            output_sents.append((sentence, self.decodeTitle(outputs)))\n",
    "        model.batch_size = batch_size\n",
    "        return output_sents\n",
    "        \n",
    "    def decode(self, sentences):         \n",
    "        with tf.Session() as sess:\n",
    "            model = self.create_model(sess, True)\n",
    "            return self.decodeWithModel(sess, model, sentences)\n",
    "\n",
    "    def self_test(self):\n",
    "        \"\"\"Test the translation model.\"\"\"\n",
    "        with tf.Session() as sess:\n",
    "            print(\"Self-test for neural translation model.\")\n",
    "            # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
    "            model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2, 5.0, 32, 0.3, 0.99, num_samples=8)\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "\n",
    "            # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
    "            data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])], [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
    "            for _ in xrange(5):  # Train the fake model for 5 steps.\n",
    "                bucket_id = random.choice([0, 1])\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch(data_set, bucket_id)\n",
    "                model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_dir=\"/home/ubuntu/data/queryClickPairs/checkpoints\"\n",
    "data_train_dir=\"/home/ubuntu/data/queryClickPairs/train\"\n",
    "data_test_dir=\"/home/ubuntu/data/queryClickPairs/test\"\n",
    "vocab_dir=\"/home/ubuntu/data/queryClickPairs/vocab\"\n",
    "\n",
    "\n",
    "rnn_size = 1024\n",
    "num_layers = 2\n",
    "batch_size = 128\n",
    "src_vocab_size = 10000\n",
    "dest_vocab_size = 10000\n",
    "max_grad_clip = 2.0\n",
    "learning_rate = 0.50\n",
    "learning_rate_decay_factor = 0.99\n",
    "\n",
    "buckets = [(6, 16)] #[(5, 10), (10, 15), (20, 25), (40, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queryTitlePairLoader_train = QueryTitleLoader(data_train_dir, vocab_dir, max_query_len=6, max_title_len=16)\n",
    "queryTitlePairLoader_test = QueryTitleLoader(data_test_dir, vocab_dir, max_query_len=6, max_title_len=16)\n",
    "\n",
    "#queryVocab, titleVocab = queryTitlePairLoader_train.generateVocab(num_examples=10000000, query_vocab_size=src_vocab_size, title_vocab_size=dest_vocab_size)\n",
    "#queryTitlePairLoader_test.setVocab(queryVocab, titleVocab)\n",
    "#queryTitlePairLoader_train.saveVocab()\n",
    "\n",
    "queryTitlePairLoader_train.loadVocab()\n",
    "queryTitlePairLoader_test.loadVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queryTitleRNNGenerator = QueryTitleRNNGenerator(checkpoint_dir, queryTitlePairLoader_train, queryTitlePairLoader_test, num_layers, rnn_size, src_vocab_size, dest_vocab_size, \n",
    "                                                max_grad_clip, batch_size, learning_rate, learning_rate_decay_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from /home/ubuntu/data/queryClickPairs/checkpoints/translate.ckpt-93\n",
      "  eval: perplexity 371.77\n",
      "  eval: perplexity 5756.62\n",
      "  eval: perplexity 1322.10\n",
      "  eval: perplexity 483.96\n",
      "  eval: perplexity 3543.78\n",
      "  eval: perplexity 1142.28\n",
      "  eval: perplexity 343.76\n",
      "  eval: perplexity 1977.72\n",
      "  eval: perplexity 858.03\n",
      "  eval: perplexity 323.00\n",
      "  eval: perplexity 532.47\n",
      "  eval: perplexity 1476.60\n",
      "  eval: perplexity 1671.63\n",
      "  eval: perplexity 500.68\n",
      "  eval: perplexity 3674.89\n",
      "  eval: perplexity 842.23\n",
      "  eval: perplexity 294.83\n",
      "  eval: perplexity 1385.23\n",
      "  eval: perplexity 530.18\n",
      "global step 113 learning rate 0.4257 step-time 0.02 perplexity 1.41\n",
      "results: \n",
      "[1 6 female                    ]  [1 6 female figure super flexible suntan ]  out:[old old monitor monitor monitor tote cer]\n",
      "[1 6 female                    ]  [asmus toys 1 6 lord of the rings hobt01 ]  out:[old old monitor monitor monitor tote cer]\n",
      "[1 6 female                    ]  [acplay 1 6 atx014 silver net yarn splici]  out:[old old monitor monitor monitor tote cer]\n",
      "[1 6 female                    ]  [1 6 scale gentlemen prefer marilyn monro]  out:[old old monitor monitor monitor tote cer]\n",
      "[1 6 female                    ]  [1 6 play toy hb001 asian female body wit]  out:[old old monitor monitor monitor tote cer]\n",
      "  eval: perplexity 310.57\n",
      "  eval: perplexity 504.77\n",
      "  eval: perplexity 1327.88\n",
      "  eval: perplexity 580.10\n",
      "  eval: perplexity 376.30\n",
      "  eval: perplexity 493.55\n",
      "  eval: perplexity 263.79\n",
      "  eval: perplexity 1132.67\n",
      "  eval: perplexity 639.50\n",
      "  eval: perplexity 1118.01\n",
      "  eval: perplexity 903.82\n",
      "  eval: perplexity 198.14\n",
      "  eval: perplexity 1550.11\n",
      "  eval: perplexity 555.95\n",
      "  eval: perplexity 648.76\n",
      "  eval: perplexity 1686.46\n",
      "  eval: perplexity 949.51\n",
      "  eval: perplexity 489.66\n",
      "  eval: perplexity 378.81\n",
      "  eval: perplexity 821.40\n",
      "global step 133 learning rate 0.4173 step-time 0.02 perplexity 1.40\n",
      "results: \n",
      "[2 drawer cd                   ]  [2 drawer storage chest for cds office su]  out:[badge old old old gray gray gray gray gr]\n",
      "[2 foot firestick              ]  [2 foot fiberglass 1000 watt black cb rad]  out:[badge old old old ceramic ceramic cerami]\n",
      "[2 euro kelly                  ]  [2 euro monaco 2007 grace kelly en cofret]  out:[badge old gray gray ceramic ceramic cera]\n",
      "[2 drawer oak file cabinet     ]  [oak 2 drawer legal size file cabinet    ]  out:[badge sewing radio cut cut cut cut cut c]\n",
      "[2 door jeep for sale          ]  [2003 jeep wrangler                      ]  out:[denim denim tote ceramic ceramic ceramic]\n",
      "  eval: perplexity 1303.08\n",
      "  eval: perplexity 611.24\n",
      "  eval: perplexity 648.42\n",
      "  eval: perplexity 746.76\n",
      "  eval: perplexity 633.85\n",
      "  eval: perplexity 347.79\n",
      "  eval: perplexity 301.48\n",
      "  eval: perplexity 496.64\n",
      "  eval: perplexity 344.91\n",
      "  eval: perplexity 445.35\n",
      "  eval: perplexity 724.61\n",
      "  eval: perplexity 354.13\n",
      "  eval: perplexity 98648.76\n",
      "  eval: perplexity 441.18\n",
      "  eval: perplexity 1997.16\n",
      "  eval: perplexity 431.75\n",
      "  eval: perplexity 873.63\n",
      "  eval: perplexity 596.88\n",
      "  eval: perplexity 761.71\n",
      "  eval: perplexity 528.43\n",
      "global step 153 learning rate 0.3968 step-time 0.02 perplexity 1.16\n",
      "results: \n",
      "[2 person tent                 ]  [marmot grid plus tent 2 person tent very]  out:[radio radio ceramic ceramic ceramic cera]\n",
      "[2 person tent                 ]  [2 persons tent                          ]  out:[radio radio ceramic ceramic ceramic cera]\n",
      "[2 person tent                 ]  [skandika multispace 6 person modular ten]  out:[radio radio ceramic ceramic ceramic cera]\n",
      "[2 person tent                 ]  [6 x 5 rv camping tent outdoor privacy ca]  out:[radio radio ceramic ceramic ceramic cera]\n",
      "[2 person tent                 ]  [outdoor camping tent 8 person 2 room cab]  out:[radio radio ceramic ceramic ceramic cera]\n",
      "  eval: perplexity 5375.54\n",
      "  eval: perplexity 820.41\n",
      "  eval: perplexity 307.88\n",
      "  eval: perplexity 8732.54\n",
      "  eval: perplexity 741.38\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8464fde379a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqueryTitleRNNGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-7e17349b5513>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"translate.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mstep_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph)\u001b[0m\n\u001b[1;32m   1378\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text)\u001b[0m\n\u001b[1;32m   1404\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1406\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.pyc\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m   1843\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.pyc\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mSerializeToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[1;32m   1059\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[0;32m-> 1060\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1067\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mInternalSerialize\u001b[0;34m(self, write_bytes)\u001b[0m\n\u001b[1;32m   1073\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mInternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m       \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m       \u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mEncodeField\u001b[0;34m(write, value)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m       \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m       \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mRepeatedFieldSize\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlocal_VarintSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mFieldSize\u001b[0;34m(map_value)\u001b[0m\n\u001b[1;32m    358\u001b[0m       \u001b[0;31m# duplication.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m       \u001b[0mentry_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m       \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmessage_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mFieldSize\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mFieldSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtag_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocal_VarintSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mFieldSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0msize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/encoder.pyc\u001b[0m in \u001b[0;36mFieldSize\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    306\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mFieldSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m       \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtag_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlocal_VarintSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mFieldSize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/protobuf/internal/python_message.pyc\u001b[0m in \u001b[0;36mByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_byte_size_dirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_listener_for_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "queryTitleRNNGenerator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#batch = queryTitlePairLoader_train.getQueryTitlesBatch(100)\n",
    "#random.choice(tuple(batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes at least 3 arguments (4 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-70755db3ce8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqueryTitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryTitleLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_query_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_title_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetQueryTitlesBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mqueryAllTitles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueryTitles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueryAllTitles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mqueryAllTitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes at least 3 arguments (4 given)"
     ]
    }
   ],
   "source": [
    "#Test the results of some random queries\n",
    "\n",
    "queryTitles = QueryTitleLoader(data_test_dir, max_query_len=6, max_title_len=16).getQueryTitlesBatch(10000)\n",
    "queryAllTitles = {}\n",
    "for q,t in queryTitles:\n",
    "    if q not in queryAllTitles:\n",
    "        queryAllTitles[q] = set()\n",
    "    queryAllTitles[q].add(t)\n",
    "queries = [q.strip() for q in list(queryAllTitles)]\n",
    "query_test_results = queryTitleRNNGenerator.decode(queries)\n",
    "query_test_results[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
